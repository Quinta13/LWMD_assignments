{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sebaq/Documents/GitHub/LWMD_assignments')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:16.173856231Z",
     "start_time": "2023-05-24T16:21:16.173489875Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Definition of Map and Reduce functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def document_map(\n",
    "        doc_info: tuple[str, int, list[tuple[int, float]]]\n",
    ") -> list[tuple[str, tuple[str, int, list[tuple[int, float]]]]]:\n",
    "    \"\"\"\n",
    "    Mapping function\n",
    "    :param doc_info: document information is represented as a triple:\n",
    "        - doc-id, represented as a string\n",
    "        - term-threshold, referring to the index of a specific column up to which do not map terms\n",
    "        - document vector, a sparse vector as a list of pairs (column, value) for each non-zero entries,\n",
    "            where the column is actually a term-id\n",
    "    :return: list of key-value pairs:\n",
    "        - key: term-id, which is actually a column index\n",
    "        - value: consists of a triple:\n",
    "            - doc-id  (the same as input)\n",
    "            - term-id (the same as the key)\n",
    "            - document vector (the same as input)\n",
    "    \"\"\"\n",
    "\n",
    "    # unpacking\n",
    "    doc_id: str\n",
    "    term_threshold: int\n",
    "    sparse_entries: list[tuple[int, float]]\n",
    "    doc_id, term_threshold, sparse_entries = doc_info\n",
    "\n",
    "    mapped: list[tuple[str, tuple[str, int, list[tuple[int, float]]]]] = [\n",
    "\n",
    "        (str(term_id), (doc_id, term_id, sparse_entries))\n",
    "        for term_id, value in sparse_entries  # document terms by using non-zero entries\n",
    "        if term_id > term_threshold  # OPTIMIZATION 1:\n",
    "        # we only map term with higher term-id with respect to the threshold one\n",
    "        #  (thus, we only consider columns after the threshold one)\n",
    "    ]\n",
    "\n",
    "    return mapped"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:21.270713204Z",
     "start_time": "2023-05-24T16:21:21.244822331Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def documents_reduce(docs: list[tuple[int, int, list[tuple[int, float]]]]) -> list[tuple[tuple[int, int], float]]:\n",
    "    \"\"\"\n",
    "    Reduce function\n",
    "    :param docs: list of triplets:\n",
    "        - doc-id\n",
    "        - term-id (actually a column index of the vector)\n",
    "        - document vector as a sparse matrix of pairs (column, value)\n",
    "    :return: list of tuples:\n",
    "        - the first element is the pair of documents represented by their doc-id\n",
    "        - the second element represent their cosine-similarity\n",
    "    \"\"\"\n",
    "\n",
    "    # list of output pairs\n",
    "    pairs = []\n",
    "\n",
    "    # total number of documents\n",
    "    n_docs = len(docs)\n",
    "\n",
    "    # loop among all possible pairs\n",
    "    for i in range(n_docs - 1):\n",
    "\n",
    "        for j in range(i + 1, n_docs):\n",
    "\n",
    "            doc1_id, term_id, doc1 = docs[i]\n",
    "            doc2_id, _, doc2 = docs[j]  # since the operation is an aggregation by key,\n",
    "            # term_id is expected to be the same\n",
    "\n",
    "            # ----------------- OPTIMIZATION 2 -----------------\n",
    "\n",
    "            # collect term-ids of each document\n",
    "            terms_1: list[int] = [t_id1 for t_id1, _ in doc1]  # term-ids for the first document\n",
    "            terms_2: list[int] = [t_id2 for t_id2, _ in doc2]  # term-ids for the second document\n",
    "\n",
    "            # perform their intersection\n",
    "            common_terms: set[int] = set(terms_1).intersection(terms_2)\n",
    "\n",
    "            # get the maximum term-id\n",
    "            max_term: int = max(common_terms)\n",
    "\n",
    "            # if the maximum term-id is not the same of aggregation key, skip similarity computation\n",
    "            if term_id != max_term:\n",
    "                pass\n",
    "\n",
    "            # --------------------------------------------------\n",
    "\n",
    "            # Computing similarity with dot-product\n",
    "\n",
    "            # getting iterator\n",
    "            iter_doc1 = iter(doc1)\n",
    "            iter_doc2 = iter(doc2)\n",
    "\n",
    "            # we assume documents with at least on term\n",
    "            term1, value1 = next(iter_doc1)\n",
    "            term2, value2 = next(iter_doc2)\n",
    "\n",
    "            sim = 0.  # total similarity\n",
    "\n",
    "            # we use iterators to keep a pointer over term-ids of the two vectors\n",
    "            # if they have the same term-id, we add its contribution to the cumulative sum and we move both pointers over\n",
    "            # otherwise we move over the one with smallest term-id\n",
    "\n",
    "            while True:\n",
    "\n",
    "                try:\n",
    "                    if term1 == term2:  # they have common term-id; we add its contribution to final similarity\n",
    "                        sim += value1 * value2\n",
    "                        term1, value1 = next(iter_doc1)\n",
    "                        term2, value2 = next(iter_doc2)\n",
    "                    elif term1 < term2:  # the first one has a smaller term-id\n",
    "                        term1, value1 = next(iter_doc1)\n",
    "                    else:  # the second one has a smaller term-id\n",
    "                        term2, value2 = next(iter_doc2)\n",
    "                except StopIteration:  # we scanned all terms of one of the vectors so there's no more term in common\n",
    "                    break\n",
    "\n",
    "            # we add the pairwise similarity to final output\n",
    "            pairs.append(((doc1_id, doc2_id), sim))\n",
    "\n",
    "    return pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:21.965173422Z",
     "start_time": "2023-05-24T16:21:21.909299817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from assignment3.utils import jaccard\n",
    "from assignment3.io_ import load_evaluation, get_exact_solution_file\n",
    "from typing import List, Tuple\n",
    "\n",
    "def compare_with_exact(data_name: str, collected_: List[Tuple[str, str]]) -> float:\n",
    "    \"\"\"\n",
    "    Compares results coming from spark to sequential execution\n",
    "    :param data_name: name of dataset\n",
    "    :param collected_: pairs of similar docs from spark\n",
    "    :return: jaccard similarity with exact solution\n",
    "    \"\"\"\n",
    "\n",
    "    exact = load_evaluation(path_=get_exact_solution_file(data_name=data_name))['pairs']\n",
    "    exact = [(a, b) for a, b in exact]\n",
    "\n",
    "    return jaccard(set(collected_), set(exact))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:25.665764934Z",
     "start_time": "2023-05-24T16:21:23.695302072Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation over small example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "DATA_NAME = 'small'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:29.181306784Z",
     "start_time": "2023-05-24T16:21:29.151685179Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "SIMILARITY = 0.8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:29.851993959Z",
     "start_time": "2023-05-24T16:21:29.813783013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "IDF_ORDER = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:30.543989744Z",
     "start_time": "2023-05-24T16:21:30.510591376Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading document info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors... \n",
      "Loading mapping... \n",
      "Loading inverse mapping... \n"
     ]
    }
   ],
   "source": [
    "from assignment3.model.documents import DocumentVectors\n",
    "docs_vet = DocumentVectors(data_name=DATA_NAME, idf_order=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:32.294067065Z",
     "start_time": "2023-05-24T16:21:32.181228017Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "small Vector Documents [4735]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:33.168413105Z",
     "start_time": "2023-05-24T16:21:33.040944767Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "docs_info = docs_vet.get_documents_info(similarity=SIMILARITY)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:36.520432585Z",
     "start_time": "2023-05-24T16:21:33.839467507Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(docs_info)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:38.446638926Z",
     "start_time": "2023-05-24T16:21:37.687465688Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "out = rdd.flatMap(document_map).\\\n",
    "    combineByKey(lambda x: [x], lambda x, y: x + [y], lambda x, y: x + y).\\\n",
    "    flatMapValues(documents_reduce).\\\n",
    "    filter(lambda x: x[1][1] > SIMILARITY).\\\n",
    "    map(lambda x: x[1][0]).\\\n",
    "    distinct()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T16:21:38.819413721Z",
     "start_time": "2023-05-24T16:21:38.527561355Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 18:21:40 WARN TaskSetManager: Stage 0 contains a task of very large size (2112 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    }
   ],
   "source": [
    "collected = out.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-24T16:21:39.710044165Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[('doc0', 'doc3'), ('doc1', 'doc3'), ('doc0', 'doc1')]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:36:43.171621021Z",
     "start_time": "2023-05-23T14:36:43.135997701Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_with_exact(data_name=DATA_NAME, collected_=collected)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation over medium example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "DATA_NAME = 'medium'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SIMILARITY = 0.85"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDF_ORDER = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading document info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from assignment3.model.documents import DocumentVectors\n",
    "docs_vet = DocumentVectors(data_name=DATA_NAME, idf_order=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "toy Vector Documents [10]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "docs_info = docs_vet.get_documents_info(similarity=SIMILARITY)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(docs_info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "out = rdd.flatMap(document_map).\\\n",
    "    combineByKey(lambda x: [x], lambda x, y: x + [y], lambda x, y: x + y).\\\n",
    "    flatMapValues(documents_reduce).\\\n",
    "    filter(lambda x: x[1][1] > SIMILARITY).\\\n",
    "    map(lambda x: x[1][0]).\\\n",
    "    distinct()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "collected = out.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[('doc0', 'doc3'), ('doc1', 'doc3'), ('doc0', 'doc1')]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_with_exact(data_name=DATA_NAME, collected_=collected)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation over large example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "DATA_NAME = 'large'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SIMILARITY = 0.9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDF_ORDER = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading document info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from assignment3.model.documents import DocumentVectors\n",
    "docs_vet = DocumentVectors(data_name=DATA_NAME, idf_order=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "toy Vector Documents [10]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "docs_info = docs_vet.get_documents_info(similarity=SIMILARITY)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(docs_info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "out = rdd.flatMap(document_map).\\\n",
    "    combineByKey(lambda x: [x], lambda x, y: x + [y], lambda x, y: x + y).\\\n",
    "    flatMapValues(documents_reduce).\\\n",
    "    filter(lambda x: x[1][1] > SIMILARITY).\\\n",
    "    map(lambda x: x[1][0]).\\\n",
    "    distinct()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "collected = out.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[('doc0', 'doc3'), ('doc1', 'doc3'), ('doc0', 'doc1')]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_with_exact(data_name=DATA_NAME, collected_=collected)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
